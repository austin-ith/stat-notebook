---
title: "Math 425 Logistic Regression"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r}
library(tidyverse)

grades425 <- read.csv("Math425HistoricGrades.csv")
```

```{r}

grades425 <- grades425 %>% 
  mutate(FinalGrade = ifelse(Final.Letter.Grade %in% c("A","A-"), 1, 0))

pairs(grades425[,c(29,24,11:17)], panel = panel.smooth)

```

## Har.Work.2 Looks Promising

```{r}

glm1 <- glm(FinalGrade == 1 ~ Hard.Work.2, data = grades425, family = binomial)
summary(glm1) #AIC: 81.082

glm1 <- glm(FinalGrade == 1 ~ Hard.Work.7, data = grades425, family = binomial)
summary(glm1) #AIC: 26.343

```

We use the AIC in logistic regression to select the "best" model. AIC stands for Akaike Information Criterion. Akaike was some guy that came up with this thing.

Lower is better(-infinity is best positive infinity is worst)










