---
title: "Theory Assignment - Sampling Distributions Unveiled" 
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

```{r setup, include=FALSE}
library(mosaic)
library(tidyverse)
library(ggplot2)
library(car)
library(pander)
library(DT)
library(gridExtra)
library(formatR)

```

## {.tabset}

Using simulation, I'm going to support and demostrate the following concepts:

1. Sampling Distributions
    i. What is a sampling distribution?
    ii.  How do you obtain a sampling distribution for slope and intercept estimates in regression?
    
2. P-values
    i. How do we obtain the p-values for the test of hypotheses about $\beta_0$ and $\beta_1$ for both slope and intercept estimates using the standard errors of the sampling distributions?
    
3. Confidence Intervals
    i. How do we obtain confidence intervals for the true regression intercept, $\beta_0$, and the true regression slope, $\beta_1$, using the standard errors of the sampling distributions?

To help understand each of these concepts, we'll be frequently referring to the regression model:
$$
Y_i = \overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope}X_i + \epsilon_i \quad where \ \epsilon_i \sim \ N(0, \sigma^2)
$$
\

### Sampling Distributions 

\

To put it simply, a Sampling Distribution is a distribution of any type of statistic that we get from drawing [all possible] a large number of samples from a population. 

I'm going to demostrate a sampling distribution by generating slope and intercept estimates through a simulation. The simulation will create a scenerio where we control the ACTUAL $\beta_0$, $\beta_1$, and the Residual Standard Error($\sigma$). I'll then use the simulation to show how close we can estimate those values.

#### Setting the Stage
```{r}
N <- 500
n <- 40

storage_b0 <- rep(NA, N)
storage_b1 <- rep(NA, N)

for (i in 1:N){
  Xi <- rep(seq(30, 100, length.out=n/2), each=2)
  Yi <- 8 + 2.7*Xi + rnorm(n, 0, 2.5)
  mylm <- lm(Yi~Xi)
  coef(mylm)
  storage_b0[i] <- coef(mylm)[1]
  storage_b1[i] <- coef(mylm)[2]
}

sigma = 2.5

sigma_sq_b0 <- sigma * ((1/n) + (mean(Xi)/(sum((Xi-mean(Xi))^2))))
sigma_b0 <- sqrt(sigma_sq_b0)

sigma_sq_b1 <- sigma/sum((Xi-mean(Xi))^2)
sigma_b1 <- sqrt(sigma_sq_b1)

sim <- data.frame(b_0 = storage_b0, b_1 = storage_b1)
data1 <- data.frame(x = Xi, y = Yi)

```

Using the equation in the introduction section, lets go ahead and arbitrarily set the ACTUAL values now:
<center>
$\beta_0=8$ 

$\beta_1=2.7$

$\sigma=2.5$
</center>
$X_i$ will be a set of evenly spaced numbers between the arbitrarily set values of 25 and 125. With these values we will be taking the random samples using the equation:
<center>
$$
  Y_i = \overbrace{\beta_0}^{8} + \overbrace{\beta_1}^{2.7} X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \overbrace{\sigma^2}^{\sigma=2.5})
$$
</center>

Now for the simulation, we need two variables, n and N. 

n = the random sample size that we'll be taking.

N = the number of times we will take that random sample and record its output.

We will set those to: 

$$
n = 40
$$
$$
N = 500
$$

The final things are related to R, we will create two storage variables, storage_b0, and storage_b1. These will contain all of the coefficents to results obtained by running each sample through a linear regression. 

#### One Last Thing

Now since we are estimating $\beta_0$ and $\beta_1$, lets also estimate $\sigma_{\beta_0}^2$ and $\sigma_{\beta_1}^2$, or the variance of $\beta_0$ and $\beta_1$. To do that we'll need to calculate the variance of the sampling distribution, $b_0$, using the equation:

$$
\sigma_{b_0}^2 = \sigma^2(\frac{1}{n} + \frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}) = `r sigma` (\frac{1}{`r n`} + \frac{`r mean(Xi)`^2}{`r sum((Xi-mean(Xi))^2)`})
$$

The square-root of this variance gives the standard error of the sampling distribution of $b_0$:
<center>
$\sigma_{b_0} = {`r sigma_b0`}$
</center>
\

And the variance of the sampling distribution, $b_1$, using the equation:

$$
\sigma_{b_1}^2 = \frac{\sigma^2}{\sum(X_i-\bar{X})^2} = \frac{`r sigma`}{`r sum((Xi-mean(Xi))^2)`}
$$

The square-root of this variance gives the standard error of the sampling distribution of $b_1$:
<center>
$\sigma_{b_1} = {`r sigma_b1`}$
</center>

\

#### The Simulation

The simulation will now loop throught the following process 500 times:

 * Create a random sample of 40 
 * Run that sample data through a regression
 * Store the regression coefficents in a list.
 
A table displaying a snippet of the output is shown:

```{r echo=TRUE, tidy = TRUE}
datatable(sim, options=list(lengthMenu = c(5,10,25)))
```

Using this data, the distributions of both $b_0$ and $b_1$ are shown in the histograms below:

<center>
```{r}

hist1 <- ggplot(data = sim, aes(x = b_0)) +
  geom_histogram(aes(y = ..density..), binwidth = .5, fill = "skyblue", color = "black") +
  theme_bw() +
  labs(x = expression(b[0]),
       title = "Distribution of Intercept Values") +
  theme(plot.title = element_text(hjust = .6)) +
  scale_x_continuous(breaks = c(2,4,6,8,10,12,14)) +
  stat_function(fun = dnorm, args = list(mean = mean(sim$b_0), sd = sd(sim$b_0)), size = 1)

hist2 <- ggplot(data = sim, aes(x = b_1)) +
  geom_histogram(aes(y = ..density..), binwidth = .01, fill = "skyblue", color = "black") +
  theme_bw() +
  labs(x = expression(b[1]),
       title = "Distribution of Slope values") +
  theme(plot.title = element_text(hjust = .5)) +
  stat_function(fun = dnorm, args = list(mean = mean(sim$b_1), sd = sd(sim$b_1)), size = 1)
  
grid.arrange(hist1, hist2, ncol=2)

```
</center>

Now this is super interesting. If you recall, Our chosen values of $\beta_0$ and $\beta_1$ were 8 and 2.7 respectively. The histograms show the data normal distributed around those two values. To confirm what the histograms are showing, see the data table below. 

<center>
```{r}
sim0 <- data.frame(b = storage_b0)
sim0 <- sim0 %>% 
  mutate(
    distribution = case_when(
      !is.na(b) ~ "Intercept"
      )
  )
sim1 <- data.frame(b = storage_b1)
sim1 <- sim1 %>% 
  mutate(
    distribution = case_when(
      !is.na(b) ~ "Slope"
      )
  )

sim <- rbind(sim0, sim1)
sim$distribution <- as.factor(sim$distribution)

sim %>% 
  group_by(distribution) %>% 
  summarise("Mean" = mean(b),
            "Standard Deviation" = sd(b)) %>% 
  pander()
```
</center>

These are amazingly close to the actually values. I'd like to point out that we calculated the standard deviation or standard error to be $\sigma_{b_1}={`r sigma_b1`}$. This is really close to value shown in the table for standard deviation or standard error of slope. Which I'll remind you, we were able to determine by knowing the X values and the true sigma squared. This means that we can work this backward, that when we do not know sigma-squared, a sampling distribution will give us an very accurate view of the true value. 

The standard errors are thus important and interesting in giving us the ability to both test hypotheses around the data and create confidence intervals as will be shown in the P-values and confidence interval tabs. 

\
\
\

### P-values

\

#### Background

As has been mentioned, when we create a regression model around data, we are almost always making inferences around both $\beta_0$ and $\beta_1$. Now, the importance in your analysis of one or the other can vary in the interests of your hypothesis. But typically, the interest lies in $\beta_1$. A reason being is that if $\beta_1$, or the slope, were equal to 0, then there would be no correlation between the X and Y variables. This situation describes one of three types of inferences we typically make. I will quote Brother Saunders in describing the three:

  "*1. Determine if there is evidence of a meaningful linear relationship in the data. If $\beta_1 = 0$, then there is no relation between $X$ and $E\{Y\}$. Hence we might be interested in testing the hypotheses*
$$
  H_0: \beta_1 = 0
$$
$$
  H_a: \beta_1 \neq 0 
$$

  *2. Determine if the slope is greater, less than, or different from some other hypothesized value. In this case, we would be interested in using hypotheses of the form*
$$
  H_0: \beta_1 = \beta_{10}
$$
$$
  H_a: \beta_1 \neq \beta_{10} 
$$
*where $\beta_{10}$ is some hypothesized number.*
  
  *3. To provide a confidence interval for the true value of $\beta_1$*"

\

This section, we'll only be addressing the first two hypthesis.

#### Calculating a p-value

Now this is pretty cool. Using our hypthesis, $b_0$, and $s_{b_0}$, we can determine the test statistic using the formula:
$$
t = \frac{b_0 - \overbrace{0}^\text{a number}}{s_{b_0}}
$$
where:
$$
  H_0\ :\ \beta_0 = \overbrace{0}^\text{a number}
$$
$$
  H_a\ :\ \beta_0 \ne \overbrace{0}^\text{a number}
$$

Using the numbers from our sampling distribution simulation
<center>
```{r}
sim %>% 
  group_by(distribution) %>% 
  summarise("Mean" = mean(b),
            "Standard Deviation" = sd(b)) %>% 
  pander()

t0 <- mean(sim0$b)/sd(sim0$b)
p_b0 <- pt(-abs(t0), n-2)*2
t1 <- mean(sim1$b)/sd(sim1$b)
p_b1 <- pt(-abs(t1), n-2)*2
```
</center>

We can now determine our test statistic, or rather the number of standard Deviations we are from our Null Hypothesis. Plugging the numbers into the equation above for the test statistic for $b_0$ we get:

$$
t_{b_0} = \frac{`r mean(sim0$b)` - 0}{`r sd(sim0$b)`} = {`r t0`}
$$
With this test statistic, we can then determine the p-value, or the probability of our test statisic, using:

$$
pt(-abs(t-value), \text{degrees of freedom})
$$

With this test statistic and 40 degrees of freedom, we'll multiply the result by two because this is a two-sided test, and we get: 
$$
\text{p-value }_{b_0}={`r p_b0`}
$$

And mimicking the same process, but with $b_1$, we get:

$$
t_{b_1} = \frac{`r mean(sim1$b)` - 0}{`r sd(sim1$b)`} = {`r t1`}
$$
With this test statistic and 40 degrees of freedom; we'll multiply the result by two and we get: 
$$
\text{p-value }_{b_1}={`r p_b1`}
$$

\

\


### Confidence Intervals
\
\

Here is a summary again of the statistics we have collected to this point:

```{r eval=FALSE, include=FALSE}
sim_all <- sim %>%
  mutate("t-value" = case_when(
    distribution %in% "Intercept" ~ t0,
    distribution %in% "Slope" ~ t1))

sim_all <- sim_all %>%
  mutate(
    "p-value" = case_when(
    distribution %in% "Intercept" ~ p_b0,
    distribution %in% "Slope" ~ p_b1
    )
  )

sim_all$"t-value" <- formatC(sim_all$"t-value",format = "e")
sim_all$"p-value" <- formatC(sim_all$"p-value",format = "e")

sim_all %>%
  group_by(distribution) %>%
  summarise(Mean = mean(b),
            "Standard Deviation" = sd(b),
            "t-value" = "t-value",
            "p-value"= "p-value") %>% 
  pander()

```

<center>
```{r}
sim %>% 
  group_by(distribution) %>% 
  summarise("Mean" = mean(b),
            "Standard Deviation" = sd(b)) %>% 
  pander()
```
</center>


Taking things one more step, we can create Confidence Intervals for $\beta_0$ and $\beta_1$ using the standard errors of the sampling distributions. Using the level of significance at $\alpha=0.05$, We can determine the confidence intervals using the following two formulas and building off the work that we have already done:

$$
  b_1 \pm t^*_{n-2}\cdot s_{b_1}
$$
$$
  b_0 \pm t^*_{n-2}\cdot s_{b_0}
$$
where $t^*_{n-2}$ is the critical value from a t distribution at 38 degrees of freedom and $\alpha=0.05$.

Calculating the critical value from a t distribution, we get:
```{r include=FALSE}
cv <- qt(((1-.05)/2), n-2)
```

$$
\text{Critical value} = `r cv`
$$

Plugging our values into the equations above, our Confidence Intervals are:

$$
 \beta_1: \quad `r mean(sim1$b)` \pm `r cv*sd(sim1$b)`
$$
$$
 \beta_0: \quad `r mean(sim0$b)` \pm `r cv*sd(sim0$b)`
$$

The confidence interval being a tool for estimating the "true" average y-value for any given x-value. To give you a sense of WHY we have 95% confidence in our model's output, lets go ahead and take a look at a plot showing our sampling distributions regression line. Now I've zoomed in on this graph to better show the gray area around on regression line. This gray area is our confidence interval. 

```{r}
min <- mean(sim1$b) + cv*sd(sim1$b)
max <- mean(sim1$b) - cv*sd(sim1$b)

n <- 250

for (i in 1:n){
  Xi <- rep(seq(30, 100, length.out=n/2), each=2)
  Yi <- 8 + 2.7*Xi + rnorm(n, 0, 20)
}

data1 <- data.frame(x= Xi, y = Yi)

ggplot(data1, aes(x = Xi, y = Yi)) +
  geom_point(alpha = .1) +
  geom_smooth(method = "lm", se = T) +
  theme_bw() +
  labs(x = "X values",
       y = "Simulated Y values",
       title = "Regression line of Sampling Distributions") +
  theme(plot.title = element_text(hjust = .5)) +
  coord_cartesian(xlim = c(60,80), ylim = c(160,240))
```

To expound on this point. the gray area can also be thought of to represent the regression lines of the samples we've pulled up to two standard deviations. Or in other words 475 of the 500 samples regression lines are shown in that gray area. The confidence interval thus becomes a powerful indicator to the accuracy of our regression line or the estimated mean y-value for any x-value. 

So, as you can clearly see, the standard errors of the sampling distributions play a huge role in determining the outcome of our regression model and helping understand its fit to the data. 

\

\

\










