---
title: "Skills Quiz: Regression Diagnostics & Transformations"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
    toc: true
    toc_float: true
---


## Instructions

Use this file to keep a record of your work as you complete the "Skills Quiz: Regression Diagnostics & Transformations" assignment in Canvas.


----
```{r message=FALSE, warning=FALSE}
library(mosaic)
library(tidyverse)
library(car)
library(pander)
```

<!-- Note: The {} after each Problem and Part header allows you to keep track of what work you have completed. Write something like {Done} once you complete each problem and your html file will then show you a nice summary of what you have "done" already. -->

## Problem 1 {}

Open the `Davis` dataset in R, found in `library(car)`. As stated in the help file for this data set, "The subjects were men and women engaged in regular exercise." 

Perform a simple linear regression of the height of the individual based on their weight.

### Part (a) {}

Type out the mathematical equation for this regression model and label both $Y$ and $X$ in the equation.

<div class="YourAnswer">

$$
  \underbrace{Y_i}_\text{Height} = \beta_0 + \beta_1\underbrace{X}_\text{Weight(kg)} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0,\sigma^2)
$$

</div>


### Part (b) {}
 
Plot a scatterplot of the data with your regression line overlaid.

<div class="YourAnswer">

```{r}

Davis %>% 
  ggplot(aes(x = weight, y = height)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Measured Weight of Individual in kg (weight)",
       y = "Measured Height of Individual in cm (height)",
       title = "Exercising Individuals (Davis data set)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5))

```

</div>


### Part (c) {}

Create a residuals vs fitted-values plot for this regression. What does this plot show?

<div class="YourAnswer">

```{r}
d.lm <- lm(height ~ weight, data = Davis)

plot(d.lm, which = 1)
```

This plot show a crazy outlier in the point 12. It also shows that the linearity is off as there the variance appears to have a pattern going up from left to right. 

</div> 
 
 
### Part (d) {}

State and interpret the slope, y-intercept, and $R^2$ of this regression. Are they meaningful for this data under the current regression?

<div class="YourAnswer">

```{r}
summary(d.lm)
```

The slope's p-value is .00715 and the intercepts p-value is less than 2e-16, which are both less than $\alpha=0.05$, therefore the slope and intercept of the graph are significant. However, a person's height being 160.09 cm at age zero is not meaningful for this study. $R^2=.03597$ which is really low(nearer 0 than 1), meaning that the variance is very high for this particular linear regression and thus our prediction base on the model may not have the greatest accuracy.

</div>


### Part (e) {}

Run `View(Davis)` in your Console. What do you notice about observation #12 in this data set? 

Perform a second regression for this data with observation #12 removed. Recreate the scatterplot of Part (b) with two regression lines showing this time. The first regression line should include the outlier. The second should exclude the outlier. Include a legend to show which line is which.

<div class="YourAnswer">

```{r}
Davis1 <- Davis %>% 
  filter(weight != 166)

ggplot(data = Davis, aes(x = weight, y = height)) +
  geom_point(data = Davis, color = "black") + 
  geom_smooth(data = Davis, method = "lm", se = FALSE, aes(color = "a")) +
  geom_smooth(data = Davis1, method = "lm", se = FALSE, aes(color = "b")) + 
  labs(x = "Measured Weight of Individual in kg (weight)",
       y = "Measured Height of Individual in cm (height)",
       title = "Exercising Individuals (Davis data set)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = .5),
        legend.position = c(0.2,.12)) +
  scale_color_manual("", values = c("black", "skyblue"), labels = c("Fitted Regression (with outlier)",
                                                              "Fitted Regression (outlier removed)"))

```

</div>


### Part (f) {}

Compute the slope, y-intercept, and $R^2$ value for the regression with the outlier removed. compare the results to the values when the outlier was present.

<div class="YourAnswer">

```{r}
d2.lm <- lm(height ~ weight, data = Davis1)
summary(d2.lm)
```

First off, the p-values for both intercept and slope are very small. Where the slope was .15, it is now .52. Where the intercept was 160.09, it is now 136.83. The most significant number, however, is where $R^2$ was .04, it is now .59. This means that the regression models fits the points better.

</div>


### Part (g)

Create a residuals vs fitted-values plot for the regression with the outlier removed. How do things look now?

<div class="YourAnswer">

```{r}
plot(d2.lm, which = 1)
```

Things look considerable better. The linearity and constant variance look good.

</div>


----

## Problem 2 {}

Open the **Prestige** data set found in `library(car)`.

Perform a regression that explains the 1971 average annual **income** from jobs according to their "Pineo-Porter **prestige** score for occupation, from a social survey conducted in the mod-1960's."

### Part (a) {}

Plot the data and fitted simple linear regression line.

<div class="YourAnswer">

```{r}
prestige.lm <- lm(income ~ prestige, data = Prestige)

Prestige %>% 
  ggplot(aes(x= prestige, y= income)) +
  geom_point(color = "green") +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  labs(x = "Prestige Ranking of Occupation (prestige)",
       y = "Average Annual Income USD (income)",
       title = "Greater Prestige linked to Greater Income\n(Prestige data set)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))


```

</div>


### Part (b) {}

State the estimated values for $\beta_0$, $\beta_1$, and $\sigma$ for this regression. 

<div class="YourAnswer">

```{r}
 summary(prestige.lm)
```

$\beta_0 = -1465.03$
$\beta_1 = 176.43$
$\sigma = 2984$


</div>


### Part (c) {}

Create a residuals vs fitted-values plot and a Q-Q Plot of the residuals for this regression. 

<div class="YourAnswer">

```{r}
par(mfrow=c(1,2))
plot(prestige.lm, which = 1)
qqPlot(prestige.lm$residuals, id = FALSE, main = "QQ Plot",
       ylab = "Residuals")
```

</div> 


### Part (d) {}

Comment on any difficulties the diagnostic plots in Part (c) reveal about the regression. 

Comment on which estimates of Part (b) are likely effected by these difficulties.

<div class="YourAnswer">

It appears that the linearity is okay. However the constant variance seems to increase as the fitted values increase which isn't good. The normality from the QQ-plot also seems to be pretty bad. 

</div> 
 

 



----


## Problem 3 {}

Open the **Burt** data set from library(car).

This data set is famous for being fraudulent, or fake. See ?Burt for more details. One of the first indicators that it was fraudulent was revealed by regressing IQbio ~ IQfoster. This regression was just a little too good to be real. (Note that for social science data, like this data, $R^2$ values above 0.3 are impressive. Values above 0.7 are rare.)

### Part (a) {}

Plot the data and fitted regression line. State the estimated values of $\beta_0$, $\beta_1$, and $\sigma$ as well as the $R^2$ of the regression.

<div class="YourAnswer">

```{r}
burt.lm <- lm(IQbio ~ IQfoster, data = Burt)

Burt %>% 
  ggplot(aes(x = IQfoster, y = IQbio)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Twin IQ, Raised by Foster Parents (IQfoster)",
       y = "Twin IQ, Raised by Biological Parents (IQbio)",
       title = "Linked IQ?\n(Burt data set)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

$\beta_0=9.21$
$\beta_1=0.90$
$R^2=0.78$


</div>



### Part (b) {}

Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot for this regression. Are any problems with regression assumption violations visible in these plots?

<div class="YourAnswer">

```{r}
par(mfrow = c(1,3))
plot(burt.lm, which = 1)
qqPlot(burt.lm$residuals, id = FALSE, main = "QQ Plot Residuals",
       ylab = "Residuals")
plot(burt.lm$residuals, main = "Residuals vs Order", xlab="",ylab="Residuals")
```

</div>


### Part (c) {}

Comment on what the three diagnostic plots of Part (b) show for the regression. 

<div class="YourAnswer">

There appears to be an issue with linearity and constant variance. The normality(QQplot) and Residuals vs Order plots look okay though.

</div>





----

## Problem 4

Open the **mtcars** data set in R.

Perform a regression of **mpg** explained by the **disp**lacement of the vehicle's engine.

### Part (a) {}

Plot the data and fitted regression line. State the estimated values of $\beta_0$, $\beta_1$, and $\sigma$ as well as the $R^2$ of the regression.

<div class="YourAnswer">

```{r}
mtcars.lm <- lm(mpg ~ disp, data = mtcars)

mtcars %>% 
  ggplot(aes(x = disp, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Engine Displacement cu. in. (disp)",
       y = "Gas Mileage (mpg)", 
       title = "Reduced Fuel Efficiency\n(mtcars data set)") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

```

$\beta_0=29.60$
$\beta_1=-0.04$
$\sigma=3.25$
$R^2=.7183$


</div>



### Part (b) {}

Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot for this regression. Are any problems with regression assumption violations visible in these plots?

<div class="YourAnswer">

```{r}
par(mfrow = c(1,3))
plot(mtcars.lm, which = 1)
qqPlot(mtcars.lm$residuals, id = FALSE, main = "Q-Q Plot Residuals", ylab = "Residuals")
plot(mtcars.lm$residuals, main = "Residuals vs Order", ylab = "Residuals")
```

</div>


### Part (c) {}

Comment on what the three diagnostic plots of Part (b) show for the validity of the values computed in Part (a). 

<div class="YourAnswer">

The linearity on this seems to be violated as the majority of the points are towards the bottom pulling the red line. Constant variance also seems to be violated as the residuals are greater as the fitted values increase. The Q-Q plot and Residuals vs Order plots seem to be fine. 

</div>







## Problem 5 {}

Open the **Orange** data set found in R.

Perform a regression that explains the **circumference** of the trunk of the orange tree as the tree **age**s.

### Part (a) {}

Plot the data and fitted simple linear regression line.

<div class="YourAnswer">

```{r}
orange.lm <- lm(circumference ~ age, data = Orange)

Orange %>% 
  ggplot(aes(x= age, y = circumference)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Age of Tree in Days",
       y = "Circumference of Tree (mm)",
       title = "Growth of Orange Trees") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

</div>


### Part (b) {}

State the estimated values for $\beta_0$, $\beta_1$, and $\sigma$ for this regression. 

<div class="YourAnswer">

```{r}
summary(orange.lm)
```
$\beta_0=17.40$
$\beta_1=0.11$
$\sigma=23.74$


</div>


### Part (c) {}

Create a residuals vs fitted-values plot and a Q-Q Plot of the residuals for this regression. 

<div class="YourAnswer">

```{r}
par(mfrow = c(1,3))
plot(orange.lm, which = 1)
qqPlot(orange.lm$residuals, id = FALSE)
plot(orange.lm$residuals)
```

</div> 


### Part (d) {}

Comment on any difficulties the diagnostic plots in Part (c) reveal about the regression. 

Comment on which estimates of Part (b) are likely effected by these difficulties.

<div class="YourAnswer">

The only difficulity arises in the constant variance. The residuals seem to increase as the fitted values increase. The QQplot and Order vs fitted values plots look good. 

</div> 
 

### Part (e) {}

Perform a Box-Cox analysis of the regression. Which Y-transformation is suggested?

<div class="YourAnswer">

```{r}
boxCox(orange.lm)
```

I'd use a lambda = 0.5$

</div> 

 
### Part (f) {}

Perform a regression with the transformed y-variable. Plot the regression in the transformed units. Diagnose the fit of the regression on the transformed data.

<div class="YourAnswer">

```{r}
orange.lm.t <- lm(sqrt(circumference) ~ age, data = Orange)
summary(orange.lm.t)

Orange %>% 
  ggplot(aes(x= age, y = circumference)) +
  geom_point() +
  coord_trans(y = "sqrt") +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Age of Tree in Days",
       y = "Circumference of Tree (mm)",
       title = "Growth of Orange Trees") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))


par(mfrow= c(1,3))
plot(orange.lm.t, which = 1)
qqPlot(orange.lm.t$residuals, id = FALSE)
plot(orange.lm.t$residuals)


```
While constant variance seems to still be violated, it is a little better. The linearity appears to be worse than before. The other plots look fine still.


</div> 


### Part (g) {}

Write out the fitted model for $\hat{Y}_i'$ and then rewrite the fitted model back in the original units.

<div class="YourAnswer">

$$
  \sqrt{\hat{Y}_i'} = 5.340801 + .005466X
$$

$$
  \hat{Y}_i = (5.340801 + .005466X)^2
$$

</div> 


### Part (h)

Plot the data in the original units. Place the transformed line, back in the original units, on this plot. 

<div class="YourAnswer">

```{r}
par(mfrow = c(1,1))

plot(circumference ~ age, data = Orange, pch = 20, col = "firebrick", cex = 1.2, las =1,
     xlab = "Age of Tree in Days",
     ylab = " Circumference of Tree (mm)",
     main = "Growth of Orange Trees")
abline(orange.lm)
curve( (5.340801 + .005466 * x)^2, add = TRUE, col = "orange")

b <- orange.lm.t$coefficients

ggplot(Orange, aes(x = age, y = circumference)) +
  geom_point() +
  stat_function(fun = function(x) (b[1] +b[2]*x)**2)

```

```{r}
orange.lm.ta <- lm(log(circumference) ~ age, data = Orange)
a <- orange.lm.ta$coefficients
orange.lm.tb <- lm(sqrt(circumference) ~ age, data = Orange)
b <- orange.lm.tb$coefficients
orange.lm.tc <- lm((circumference**2) ~ age, data = Orange)
c <- orange.lm.tc$coefficients
orange.lm.td <- lm((1/circumference) ~ age, data = Orange)
d <- orange.lm.td$coefficients
orange.lm.te <- lm(circumference ~ age, data = Orange)
e <- orange.lm.te$coefficients
orange.lm.tf <- lm((circumference^(-2)) ~ age, data = Orange)
f <- orange.lm.tf$coefficients


ggplot(Orange, aes(x = age, y = circumference)) +
  geom_point(color = "orangered") +
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw() +
  stat_function(fun = function(x) exp(a[1] +a[2]*x), color = "red") +
  stat_function(fun = function(x) (b[1] +b[2]*x)**2, color = "blue") +
  stat_function(fun = function(x) sqrt(c[1] +c[2]*x), color = "green") +
  # stat_function(fun = function(x) 1/(d[1] +d[2]*x), color = "orange") +
  stat_function(fun = function(x) (e[1] +e[2]*x), color = "purple") +
  stat_function(fun = function(x) exp(g[1] +g[2]*x), color = "red") +
  stat_function(fun = function(x) (h[1] +h[2]*x)**2, color = "blue") +
  stat_function(fun = function(x) sqrt(i[1] +i[2]*x), color = "green") +
  # stat_function(fun = function(x) 1/(j[1] +j[2]*x), color = "orange") +
  stat_function(fun = function(x) (k[1] +k[2]*x), color = "purple")
```

```{r}
YoungOrange <- Orange %>% 
  filter(age<1200)


orange.lm.tg <- lm(log(circumference) ~ age, data = YoungOrange)
g <- orange.lm.tg$coefficients
orange.lm.th <- lm(sqrt(circumference) ~ age, data = YoungOrange)
h <- orange.lm.th$coefficients
orange.lm.ti <- lm((circumference**2) ~ age, data = YoungOrange)
i <- orange.lm.ti$coefficients
orange.lm.tj <- lm((1/circumference) ~ age, data = YoungOrange)
j <- orange.lm.tj$coefficients
orange.lm.tk <- lm(circumference ~ age, data = YoungOrange)
k <- orange.lm.tk$coefficients
orange.lm.tl <- lm((circumference^(-2)) ~ age, data = YoungOrange)
l <- orange.lm.tl$coefficients


ggplot(cars, aes(x = speed, y = dist)) +
  geom_point(color = "orangered") +
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw() +
  stat_function(fun = function(x) (b[1] +b[2]*x)**2, color = "blue")

```



</div> 


----








<style>

.YourAnswer {
  color: #317eac;
  padding: 10px;
  border-style: solid;
  border-width: 2px;
  border-color: skyblue4;
  border-radius: 5px;
}

</style>

 
 