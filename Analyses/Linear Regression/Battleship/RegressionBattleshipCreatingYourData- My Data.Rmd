---
title: "Regression Battleship - Creating your Data"
author: "Austin Ith"
output: 
  html_document:
    theme: cerulean
---
```{r}
library(tidyverse)
```

## Creating your Data

Remember the rules...

### Rules

1. Your csv must contain 11 columns of data.
    * The first column must be your (1) Y-variable (labeled as "Y").
    * The other ten columns must be (10) X-variables (labeled as "X1", "X2", ... , "X10").
    
2. Your Y-variable (or some transformation of the Y-variable) must have been created from a linear regression model using only X-variables (or transformations of those X-variables) from within your data set.
    * Be very careful with transformations. You must ensure that you do not break the rules of a linear regression if you choose to use transformations.
    * If you choose transformations, only these functions are allowed when transforming X and Y variables: 1/Y^2, 1/Y, log(Y), sqrt(Y), Y^2, Y^3, 1/X^2, 1/X, log(X), sqrt(X), X^2, X^3, X^4, X^5. Don't forget to check Rule #3 carefully if you choose transformations.
    
3. Your sample size must be sufficiently large so that when the true model is fit to your data using lm(...), all p-values of X-variable terms (not including the intercept) found in the summary(...) are significant.

4. Must be 2D drawable.


### True Model

Write out your final "true" model in mathematical form. Make sure it matches your code.

$$
Y_i^{-2} = \beta_0 + \beta_1 X_{3i}^2 + \beta_2 X_{9i} + \beta_3 X_{3i}^2X_{9i} + \beta_4X_{3i}^3X_{9i} + \beta_5X_{3i}^4 X_{9i} + \beta_6X_{10i} + \beta_7X_{3i}^2X_{10i}  + \epsilon_i
$$

 
### The Code to Make the Data

```{r}
set.seed(750) #This ensures the randomness is the "same" everytime if you play the entire R-chunk as one entire piece of code. If you run lines separately, your data might not come out the same every time.

## To begin, decide on your sample size. (You may have to revise it later to ensure all values in your lm(...) are significant.)
  
 n <- 65
  
## Then, create 10 X-variables using functions like rnorm(n, mean, sd), rchisq(n, df), rf(n, df1, df2), rt(n, df), rbeta(n, a, b), runif(n, a, b) or sample(c(1,0), n, replace=TRUE)...

 X1 <- runif(n, -5,3) #replace this
 X2 <- rchisq(n,25) #replace this
 X3 <- runif(n, -.605, .4) #replace this
 X4 <- rnorm(n, 0, sd = .46) #replace this
 X5 <- rf(n, 3, 6) #replace this
 X6 <- sample(c(1,0),n, replace = T) #replace this
 X7 <- rbeta(n, 5,10) #replace this
 X8 <- rchisq(n,2) #replace this
 X9 <- sample(c(1,0),n, replace = T) #replace this
 X10 <- sample(c(1,0),n, replace = T) #replace this
 
## Then, create betas, errors (by choosing sigma), and Y
 
beta0 <- 7
beta1 <- -3
beta2 <- -3.2
beta3 <- 3
beta4 <- 45
beta5 <- 90
beta6 <- -2.6
beta7 <- 3


 sigma <- .25 #change to whatever you want
 

 ################################
 # You CANNOT change this part:
 errors <- rnorm(n, 0, sigma)
 ################################ 
 
 #An example of how to make Y...
 # Y <-  beta0 + beta1*X1 + beta2*X2 + beta3*X4*X2 + errors
 
 Y <-  sqrt(1/(beta0 + beta1*(X3^2) + beta2*X9 + beta3*(X3^2)*X9 + beta4*(X3^3)*X9 + beta5*(X3^4)*X9 +  beta6*X10 + beta7*(X3^2)*X10 + errors))

 
 # Load your data into a data set:
 RBdata <- data.frame(Y, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10) 
 
 #Now fit your model to make sure it comes out significant:
 mylm <- lm(Y^(-2) ~ I(X3^2) + X9 + I(X3^2):X9 + I(X3^3):X9 + I(X3^4):X9 + X10 + I(X3^2):X10, data=RBdata) #edit this code
 summary(mylm) 
 #all p-values must be significant, except "(Intercept)"
pairs(RBdata, panel = panel.smooth)

b <- mylm$coefficients

plot(Y^(-2)~X3, data = RBdata)

X9 <- 0
X10 <- 0
curve(beta0 + beta1*x^2 + beta2*X9 + beta3*(x^2)*X9 + beta4*(x^3)*X9 + beta5*(x^4)*X9 +  beta6*X10 + beta7*(x^2)*X10, add = T)

X9 <- 1
X10 <- 0

curve(beta0 + beta1*x^2 + beta2*X9 + beta3*(x^2)*X9 + beta4*(x^3)*X9 + beta5*(x^4)*X9 +  beta6*X10 + beta7*(x^2)*X10, add = T)

X9 <- 0
X10 <- 1

curve(beta0 + beta1*x^2 + beta2*X9 + beta3*(x^2)*X9 + beta4*(x^3)*X9 + beta5*(x^4)*X9 +  beta6*X10 + beta7*(x^2)*X10, add = T)

X9 <- 1
X10 <- 1

curve(beta0 + beta1*x^2 + beta2*X9 + beta3*(x^2)*X9 + beta4*(x^3)*X9 + beta5*(x^4)*X9 +  beta6*X10 + beta7*(x^2)*X10, add = T)

X9 <- 0
X10 <- 0
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")

X9 <- 1
X10 <- 0
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")

X9 <- 0
X10 <- 1
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")

X9 <- 1
X10 <- 1
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")
 
```

## Identifying the Winnders

```{r}
set.seed(650)
  
 n <- 65

 X1 <- runif(n, -5,3) #replace this
 X2 <- rchisq(n,25) #replace this
 X3 <- runif(n, -.605, .4) #replace this
 X4 <- rnorm(n, 0, sd = .46) #replace this
 X5 <- rf(n, 3, 6) #replace this
 X6 <- sample(c(1,0),n, replace = T) #replace this
 X7 <- rbeta(n, 5,10) #replace this
 X8 <- rchisq(n,2) #replace this
 X9 <- sample(c(1,0),n, replace = T) #replace this
 X10 <- sample(c(1,0),n, replace = T) #replace this
 
 
beta0 <- 7
beta1 <- -3
beta2 <- -3.2
beta3 <- 3
beta4 <- 45
beta5 <- 90
beta6 <- -2.6
beta7 <- 3


 sigma <- .25 #change to whatever you want
 

 ################################
 # You CANNOT change this part:
 errors <- rnorm(n, 0, sigma)
 ################################ 
 
Y <-  sqrt(1/(beta0 + beta1*(X3^2) + beta2*X9 + beta3*(X3^2)*X9 + beta4*(X3^3)*X9 + beta5*(X3^4)*X9 +  beta6*X10 + beta7*(X3^2)*X10 + errors))

RBdata2 <- data.frame(Y, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10)
 
```

### True Model
$$
Y_i^{-2} = \beta_0 + \beta_1 X_{3i}^2 + \beta_2 X_{9i} + \beta_3 X_{3i}^2X_{9i} + \beta_4X_{3i}^3X_{9i} + \beta_5X_{3i}^4 X_{9i} + \beta_6X_{10i} + \beta_7X_{3i}^2X_{10i}  + \epsilon_i
$$
```{r}
mylm <- lm(Y^(-2) ~ I(X3^2) + X9 + I(X3^2):X9 + I(X3^3):X9 + I(X3^4):X9 + X10 + I(X3^2):X10, data=RBdata)

b <- mylm$coefficients

plot(Y^(-2)~X3, data = RBdata)

X9 <- 0
X10 <- 0
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")

X9 <- 1
X10 <- 0
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")

X9 <- 0
X10 <- 1
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")

X9 <- 1
X10 <- 1
curve(b[1] + b[2]*x^2 + b[3]*X9 + b[5]*(x^2)*X9 + b[6]*(x^3)*X9 + b[7]*(x^4)*X9 +  b[4]*X10 + b[8]*(x^2)*X10, add = T, col = "skyblue")
 
```

### Brother Saunders Guess

$$
Y_i = \beta_0 + \beta_1 X_{3i}^2 + \beta_2 X_{9i} + \beta_3 X_{3i}^2X_{9i} + \beta_4X_{3i}^3X_{9i} + \beta_5X_{3i}^4 X_{9i} + \beta_6X_{10i} + \beta_7X_{3i}^4X_{10i}  + \epsilon_i
$$
```{r}
teacher_lm <- lm(1/Y^2 ~ I(X3^2) +  
                 X9 +  I(X3^2):X9 + I(X3^3):X9 + I(X3^4):X9 +
                 X10 + I(X3^4):X10, 
               data=RBdata)

b <- teacher_lm$coefficients

plot(1/Y^2 ~ X3, data=RBdata, col=interaction(X9,X10))

x9=0;x10=0;
curve(b[1] + b[2]*x^2 + b[3]*x9 + b[4]*x10 + b[5]*x^2*x9 + b[6]*x^3*x9 + b[7]*x^4*x9 + b[8]*x^4*x10, add=TRUE, col=palette()[1])

x9=1;x10=0;
curve(b[1] + b[2]*x^2 + b[3]*x9 + b[4]*x10 + b[5]*x^2*x9 + b[6]*x^3*x9 + b[7]*x^4*x9 + b[8]*x^4*x10, add=TRUE, col=palette()[2])


x9=0;x10=1;
curve(b[1] + b[2]*x^2 + b[3]*x9 + b[4]*x10 + b[5]*x^2*x9 + b[6]*x^3*x9 + b[7]*x^4*x9 + b[8]*x^4*x10, add=TRUE, col=palette()[3])


x9=1;x10=1;
curve(b[1] + b[2]*x^2 + b[3]*x9 + b[4]*x10 + b[5]*x^2*x9 + b[6]*x^3*x9 + b[7]*x^4*x9 + b[8]*x^4*x10, add=TRUE, col=palette()[4])
```
 
### Ilya's Guess

$$
Y_i = \beta_0 + \beta_1 X_{9i} + \beta_2 X_{10i} + \beta_3 X_{3i}^3X_{9i} + \beta_4X_{3i}^4X_{9i} + \beta_5X_{3i} X_{9i}X_{10i} + \beta_6X_{3i}^2 X_{9i}X_{10i} + \beta_7X_{3i}^3 X_{9i}X_{10i} + \beta_8X_{3i}^{-3} X_{9i}X_{10i}  + \epsilon_i
$$

```{r}
RBdata <- RBdata %>% mutate(X11 = case_when(X9 == 0 & X10 == 0 ~ 1, X9 == 0 & X10 == 1 ~ 2, X9 == 1 & X10 == 0 ~ 3, X9 == 1 & X10 == 1 ~ 4)) %>% mutate(X11 = as.factor(X11)); 

mylm <- lm(Y ~ X9 + X10 + X9:I(X3^3) + X9:I(X3^4) + X3:X9:X10 + X9:X10:I(X3^2)+ X9:X10:I(X3^3) + X9:X10:I(X3^(-3)), data=RBdata)

summary(mylm) 

b <- mylm$coefficients

RBdata %>% 
  ggplot(aes(X3, Y, color=X11)) + 
  geom_point(alpha=0.7) + 
  stat_function(fun=function(x) b[1],color="red", size=1) + 
  stat_function(fun=function(x) b[1] + b[2] + b[4] * x^3 + b[5] * x^4,color="skyblue", size=1) + 
  stat_function(fun=function(x) b[1] + b[3],color="forestgreen", size=1) + 
  stat_function(fun=function(x) b[1] + b[2] + b[3] + b[4] * x^3 + b[5] * x^4 + b[6] * x + b[7]*x^2 + b[8]*x^3 + b[9]*x^(-3),color="purple", size=1) + 
  theme_bw() + 
  scale_y_continuous(limits = c(0.25,1.5))
```

### Christian's Guess
$$
Y_i^{-2} = \beta_0 + \beta_1 X_{4i} + \beta_2 X_{9i} + \beta_3 X_{10i}   + \epsilon_i
$$
```{r}
christian_lm <- lm(Y^-2 ~ X4 + X9 + X10, data = RBdata)

b <- christian_lm$coefficients

plot(Y^(-2) ~ X4, data = RBdata)

X9 <- 0
X10 <- 0
curve(b[1] + b[2]*x + b[3]*X9 + b[4]*(x^2)*X10 , add = T, col = "skyblue")

X9 <- 1
X10 <- 0
curve(b[1] + b[2]*x + b[3]*X9 + b[4]*(x^2)*X10 , add = T, col = "skyblue")

X9 <- 0
X10 <- 1
curve(b[1] + b[2]*x + b[3]*X9 + b[4]*(x^2)*X10 , add = T, col = "skyblue")

X9 <- 1
X10 <- 1
curve(b[1] + b[2]*x + b[3]*X9 + b[4]*(x^2)*X10 , add = T, col = "skyblue")

```

### The Winner
 
```{r}
rb_data_ilya <- RBdata %>% mutate(X11 = case_when(X9 == 0 & X10 == 0 ~ 1, X9 == 0 & X10 == 1 ~ 2, X9 == 1 & X10 == 0 ~ 3, X9 == 1 & X10 == 1 ~ 4)) %>% mutate(X11 = as.factor(X11)); 

rb_data_ilya2 <- RBdata2 %>% mutate(X11 = case_when(X9 == 0 & X10 == 0 ~ 1, X9 == 0 & X10 == 1 ~ 2, X9 == 1 & X10 == 0 ~ 3, X9 == 1 & X10 == 1 ~ 4)) %>% mutate(X11 = as.factor(X11)); 

mylm <- lm(Y^(-2) ~ I(X3^2) + X9 + I(X3^2):X9 + I(X3^3):X9 + I(X3^4):X9 + X10 + I(X3^2):X10, data=RBdata)

teacher_lm <- lm(1/Y^2 ~ I(X3^2) +  
                 X9 +  I(X3^2):X9 + I(X3^3):X9 + I(X3^4):X9 +
                 X10 + I(X3^4):X10, 
               data=RBdata)

ilya_lm <- lm(Y ~ X9 + X10 + X9:I(X3^3) + X9:I(X3^4) + X3:X9:X10 + X9:X10:I(X3^2)+ X9:X10:I(X3^3) + X9:X10:I(X3^(-3)), data=rb_data_ilya)

christian_lm <- lm(Y^-2 ~ X4 + X9 + X10, data = RBdata)

yh_true <- predict(mylm, newdata=RBdata2)^(-1/2)
yh_teacher <- predict(teacher_lm, newdata=RBdata2)^(-1/2)
yh_ilya <- predict(ilya_lm, newdata=rb_data_ilya2)
yh_christian <- predict(christian_lm, newdata=RBdata2)^(-1/2)

ybar <- mean(RBdata2$Y)

SSTO <- sum( (RBdata2$Y - ybar)^2 )

SSE_true <- sum( (RBdata2$Y - yh_true)^2 )
SSE_teacher <- sum( (RBdata2$Y - yh_teacher)^2 )
SSE_ilya <- sum( (RBdata2$Y - yh_ilya)^2 )
SSE_christian <- sum( (RBdata2$Y - yh_christian)^2 )

rs_true <- 1 - SSE_true/SSTO
rs_teacher <- 1 - SSE_teacher/SSTO
rs_ilya <- 1 - SSE_ilya/SSTO
rs_christian <- 1 - SSE_christian/SSTO

n <- length(RBdata2$Y)
p_true <- length(coef(mylm))
p_teacher <- length(coef(teacher_lm))
p_ilya <- length(coef(ilya_lm))
p_christian <- length(coef(christian_lm))
rsa_true <- 1 - (n-1)/(n-p_true)*SSE_true/SSTO
rsa_teacher <- 1 - (n-1)/(n-p_teacher)*SSE_teacher/SSTO
rsa_ilya <- 1 - (n-1)/(n-p_ilya)*SSE_ilya/SSTO
rsa_christian <- 1 - (n-1)/(n-rs_christian)*SSE_christian/SSTO

 
```


| Model   | $R^2$ | Adjusted $R^2$ |
|---------|-------|----------------|
| True    | `r rs_true`  | `r rsa_true` |
| Brother Saunders  | `r rs_teacher`  | `r rsa_teacher` |
| Ilya | `r rs_ilya` | `r rsa_ilya` |
| Christian | `r rs_christian` | `r rsa_christian` |


Looks like I won! just barely...

 

 