---
title: "Theory Assignment - Residuals, Sums of Squares, and R-Squared" 
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---

```{r setup, include=FALSE}
library(mosaic)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(scales)
library(car)
library(pander)
library(DT)

laptops <- read_csv("../../Data/laptops.csv")

```

## {.tabset}

I am going to be using laptop data set in order to answer the following questions:

  1. What is a residual? What use does a single residual provide within a regression analysis?
  2. What are each of SSTO, SSR, and SSE? How are they related? How do they differ? Find a way to both show and explain these values. How are they used to gain insight about data within a regression analysis?
  3. What is R-squared? How is it properly interpreted? How is it calculated? What information does it provide about a regression analysis? How does it differ from the information provided in the p-value for the slope term? How does it relate to the residual standard error?
  
### Background

To best answer the questions above, we are going to be looking at the price that I paid for my laptop in comparison to the average others paid for similiar laptops. 

I own a 2014 Apple Macbook Pro with a 2.2 GHz processor and paid $1,299 for it. So, we are going filter the data set to include just Apple's laptops. The data also contains the price but in Euro's, we'll adjust to US dollars using the current exhange rate of 1 dollar to .89 Euro's.

<center>
![Apple Laptop](laptop.jpeg)

</center>

A sample of the updated data set is found below.

```{r echo= FALSE, message=FALSE, warning=FALSE}
laptop <- read_csv("../../Data/laptops.csv")

laptop <- laptop %>% 
  filter(Company == "Apple") %>% 
  mutate(price_dollars = round(Price_euros * 1.11, 2)) %>% 
  select(c(Company, Product, TypeName,Inches, Cpu, ScreenResolution,Ram,Memory,
           OpSys,Weight, price_dollars))

wordstoremove <- c("Intel ","Core", "i5 ", "i7 ", "M ", "m3 ", "GHz")

laptop1 <- laptop %>% 
  select(Cpu)

laptop2 <- as.data.frame(sapply(laptop1, function(x)
  gsub(paste(wordstoremove, collapse = '|'), '', x)))

laptop3 <- as.numeric(as.character(laptop2$Cpu))
  
laptop <- laptop %>% 
  mutate(Cpu = laptop3)

```

```{r}
datatable(data = laptop, options=list(lengthMenu = c(5,10,25)))
```



### Part 1

#### Residuals

In order to explain what a residual is, lets take a step back take a look at what a linear regression is and the mathematical model represented by the linear regression. A linear regression is the relationship between two variables. Lets go ahead and plot the laptop data set where Cpu capability in GHz determines the price of the computer, and where $Y_i$ are the observed data points. 

<center>
```{r}
ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_point(aes(color="Y_i")) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = c(0.056,.92),
        legend.text = element_text(size=12),
        legend.background = element_rect(linetype = 1, size = .1, colour = 1)) +
  scale_color_manual(values="blue", labels=expression(Y[i]))
```
</center>

From this Scatterplot, we can see that there appears to be a correlation between the size of the CPu(GHz) and the price of the computer. Creating a linear regression, we are now going to assume that we can create a mathematical equation that can predict the price from just the CPu size where $\beta_0$ and $\beta_1$ are the y-intercept and slope respectively. See equation below. 

$$
\underbrace{Y_i}_\text{Price of Computer} = \beta_0 + \beta_1\underbrace{X_i}_\text{CPU(GHz)} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0,\sigma^2)
$$

The mathematical equation above represents a perfect model that be able to predict perfectly all of $Y_i$. Since this is not possible, we'll do the next best thing and create a best-fit line using the following model, where, $\hat{Y}_i$ is the average price per CPU size. See equation below. 

$$
\underbrace{\hat{Y}_i}_\text{Estimated Price of Computer} = \overbrace{b_0 + b_1 \underbrace{X_i}_\text{CPU(GHz)}}^\text{estimated regression equation}
$$

When plotted onto the graph, the equation above gives the regression shown.

<center>
```{r message=FALSE, warning=FALSE}
ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_point(aes(color='data', shape = 'data', linetype= 'data')) +
  geom_smooth(method = "lm", se = FALSE, aes(color = 'fitted',shape = 'fitted', linetype= 'fitted')) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = c(0.057,.89),
        legend.text = element_text(size=12),
        legend.background = element_rect(linetype = 1, size = .1, colour = 1)) +
  scale_color_manual('', values=c("blue", "firebrick1"), labels=c(expression(Y[i]), expression(hat(Y)[i]))) +
  scale_shape_manual('', values = c(19,NA), labels=c(expression(Y[i]), expression(hat(Y)[i]))) +
  scale_linetype_manual('', values=c(0, 1), labels=c(expression(Y[i]), expression(hat(Y)[i])))
```
</center>

Okay, so as you can see, we now have a model $\hat{Y}_i$ that estimates the Price of the computer for each value of CPU size. Also evident is that virtually none of the $Y_i$ points are on that line. That vertical distance between the $Y_i$ and $\hat{Y}_i$ is a residual.

<center>
```{r message=FALSE, warning=FALSE}
laptop.lm <- lm(price_dollars ~ Cpu, data = laptop)

y2 <- predict(laptop.lm, data.frame(Cpu = 2.2))

ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_point(aes(color='data', shape = 'data', linetype= 'data'), size =3.5) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= predict(laptop.lm, x = Cpu), yend = price_dollars,
                   color='segment legend', shape = 'segment legend', linetype= 'segment legend'), 
               size = 2, data = laptop) +
  geom_segment(aes(x = 1.5, xend = 2.15, y=2650, yend=2150), color = "black", show.legend = FALSE, arrow = arrow(length = unit(0.3, "inches"))) +
  geom_smooth(method = "lm", se = FALSE, aes(color = 'fitted',shape = 'fitted', linetype= 'fitted')) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = c(0.12,.85),
        legend.text = element_text(size=10),
        legend.background = element_rect(linetype = 1, size = .1, colour = 1)) +
  scale_color_manual('', values=c("blue", "firebrick1","darkgreen"), labels=c(expression(Y[i]), expression(hat(Y)[i]),"Residual")) +
  scale_shape_manual('', values = c(19,NA,95), labels=c(expression(Y[i]), expression(hat(Y)[i]),"Residual")) +
  scale_linetype_manual('', values=c(0, 1, 1), labels=c(expression(Y[i]), expression(hat(Y)[i]),"Residual"))
```
</center>

This distance can be represented by the equation where r_i is the residual for all values of i:
$$
r_i = Y_i - \hat{Y}_i
$$

Now that we have determined what a residual is, we can now ask the question, what does a single residual provide within a regression analysis. To answer this, lets plot my computer, with its CPU size and cost in with all the data we compiled to this point. 

<center>
```{r message=FALSE, warning=FALSE}


y3 <- data.frame(x = c(2.2,2.2), y=c(1299,y2))

ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_point(aes(color='a', shape = 'a', linetype= 'a'), size =3.5) +
  geom_segment(aes(x = 2.2, xend = 2.2, y= y2, yend = 1299,
                   color='c', shape = 'c', linetype= 'c'), 
               size = 2, data = laptop) +
  geom_smooth(method = "lm", se = FALSE, aes(color = 'b',shape = 'b', linetype= 'b')) +
  geom_point(data = y3,aes(x = x, y = y), color = "goldenrod4", show.legend = FALSE, size =5) +
  geom_text(aes(x = 2.2, y = 1299, label="My Computer\n$1299"),nudge_y=-125,nudge_x=0, size = 4) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = c(0.12,.85),
        legend.text = element_text(size=10),
        legend.background = element_rect(linetype = 1, size = .1, colour = 1)) +
  scale_color_manual('', values=c("blue", "firebrick1", "darkgreen"), 
                     labels=c(expression(Y[i]),expression(hat(Y)[i]), "Residual")) +
  scale_shape_manual('', values = c(19,NA,95), 
                     labels=c(expression(Y[i]),expression(hat(Y)[i]), "Residual")) +
  scale_linetype_manual('', values=c(0, 1, 1), 
                        labels=c(expression(Y[i]),expression(hat(Y)[i]), "Residual"))

```
</center>

The residual for my computer being:

$$
r=1299 - 1876.51 = -577.51
$$
From this residual, I can determine that I paid $577.51 less than the average cost of a computer of the same size CPU. 

Residuals are powerful and can tell us more, as will be discussed in parts 2 and 3



### Part 2

To build off of the discussion of residuals in part 1, We are now going to introduce how we can determine the fit of a regression using residuals. The best way is to understand the fit of a regression is $R^2$. $R^2$, however, will be discussed in Part 3 as there are some building blocks we need to discuss in order to understand what $R^2$ is and means. 

#### Sums of Squares {.tabset}

There are three sums of squares: SSE, SSR, and SSTO. 

Each of these relate to one and other and provide information about how ell the regression fits the data.
 
##### SSE (Sum of Squared Errors)

The SSE measures the square of how much the residuals deviate from the line. 

The equation is given by:

$$
SSE = \sum_\text{i=1}^n(Y_i-\hat{Y}_i)^2
$$

The graph below represents an example we've already seen before, with SSE being the sum of all the residuals shown squared.

<center>
```{r message=FALSE, warning=FALSE}
laptop.lm <- lm(price_dollars ~ Cpu, data = laptop)

y2 <- predict(laptop.lm, data.frame(Cpu = 2.2))

ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_point(aes(color='data', shape = 'data', linetype= 'data'), size =3.5) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= predict(laptop.lm, x = Cpu), yend = price_dollars,
                   color='segment legend', shape = 'segment legend', linetype= 'segment legend'), 
               size = 2, data = laptop) +
  geom_smooth(method = "lm", se = FALSE, aes(color = 'fitted',shape = 'fitted', linetype= 'fitted')) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = c(0.12,.85),
        legend.text = element_text(size=10),
        legend.background = element_rect(linetype = 1, size = .1, colour = 1)) +
  scale_color_manual('', values=c("blue", "firebrick1","darkgreen"), labels=c(expression(Y[i]), expression(hat(Y)[i]),"Residual")) +
  scale_shape_manual('', values = c(19,NA,95), labels=c(expression(Y[i]), expression(hat(Y)[i]),"Residual")) +
  scale_linetype_manual('', values=c(0, 1, 1), labels=c(expression(Y[i]), expression(hat(Y)[i]),"Residual"))
```
</center>

The following equation can also be used if SSTO and SSR are known:
$$
SSE = SSTO - SSR
$$

```{r}
SSE <-  sum((laptop$price_dollars - laptop.lm$fit)^2)
```


In this particular example. The SSE = 3,302,203



##### SSR (Sum of Squares Regression)

The SSR measures the square of how much the regression line deviates from the average y-value. 

The equation is given by:

$$
SSR = \sum_\text{i=1}^n(\hat{Y}_i - \bar{Y})^2
$$

The graph below represents an example of taking the estimated price for each CPU size and subracting it from the total mean of all computer prices.

<center>
```{r message=FALSE, warning=FALSE}
laptop.lm <- lm(price_dollars ~ Cpu, data = laptop)

y2 <- predict(laptop.lm, data.frame(Cpu = 2.2))

ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= predict(laptop.lm, x = Cpu), yend = mean(price_dollars)), 
               size = 2, data = laptop) +
  geom_smooth(method = "lm", se = FALSE, aes(color = 'a', linetype= 'a')) +
  geom_line(aes(y = mean(price_dollars), color = "b", linetype = "b"), size = 1.5) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = c(0.12,.85),
        legend.text = element_text(size=10),
        legend.background = element_rect(linetype = 1, size = .1, colour = 1)) +
  scale_color_manual('', values=c("firebrick1","darkgray"), labels=c(expression(hat(Y)[i]), expression(bar(Y)))) +
  scale_linetype_manual('', values=c(1,2), labels=c(expression(hat(Y)[i]), expression(bar(Y))))
```
</center>

The following equation can also be used if SSTO and SSE are known:
$$
SSR = SSTO - SSE
$$

```{r}
SSR <-  sum((laptop.lm$fit - mean(laptop$price_dollars))^2)
```


In this particular example, SSR = 4,470,420. It is also important to note that SSR is a good indicator of the slope's significance. As you can see from the example above, the greater the value of SSR, the greater the slope. Conversely, the lower the value of SSR, the lesser the slope.


##### SSTO (Total Sum of Squares)

The SSTO measures the square of how much the y-values deviate from the average y-value. 

The equation is given by:

$$
SSTO = \sum_\text{i=1}^n(Y_i - \bar{Y})^2
$$

The graph below represents an example of taking the actual price for each CPU size and subracting it from the total mean of all computer prices.

<center>
```{r message=FALSE, warning=FALSE}
ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_point(aes(color='a', linetype= 'a'), size =3.5) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= mean(price_dollars), yend = price_dollars), 
               size = 1, data = laptop, linetype = "dashed", color = "darkgray") +
  geom_line(aes(y = mean(price_dollars), color = "b", linetype = "b"), size = 1.5) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = c(0.12,.85),
        legend.text = element_text(size=10),
        legend.background = element_rect(linetype = 1, size = .1, colour = 1)) +
  scale_color_manual('', values=c("blue", "black"), 
                     labels=c(expression(Y[i]), expression(bar(Y)))) +
  scale_linetype_manual('', values=c(0, 2), 
                        labels=c(expression(Y[i]), expression(bar(Y)))) +
  scale_shape_manual('', values = c(0,95),labels=c(expression(Y[i]), expression(bar(Y))))
```
</center>

The following equation can also be used if SSE and SSR are known:
$$
SSTO = SSE + SSR
$$

```{r}
SSTO <-  sum((laptop$price_dollars - mean(laptop$price_dollars))^2)
```


In this particular example. The SSR = 7,772,624

##### Relationship

As has been mention; SSE, SSR, and SSTO together show how well the regression fits the model. 

To illistrate what these values mean. Observe the two graphs below. The first graph represents the data that we've already been working with. The second graph has manipulated data to help illistrate what these numbers do as the points better fit the line.


<center>
```{r message=FALSE, warning=FALSE, echo = FALSE}

# First Graph

plot1 <- ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= predict(laptop.lm, x = Cpu), yend = price_dollars,
                   color='a', shape = 'a', linetype= 'a'), 
               size = 1, data = laptop) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= mean(price_dollars), yend = price_dollars,
                   color='aa', shape = 'aa', linetype= 'aa'), 
               size = 1, data = laptop) +
  geom_smooth(method = "lm", se = FALSE, aes(color = 'b',shape = 'b', linetype= 'b')) +
  geom_line(aes(y = mean(price_dollars), color = "c", shape = "c", linetype = "c"), size = 1.5) +
  geom_point(aes(color='d', shape = 'd', linetype= 'd'), size =2.5) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual('', values=c("darkgreen","darkgray" ,"firebrick1","darkgray", "blue"), 
                     labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_shape_manual('', values = c(95,95,NA,95,19), 
                      labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_linetype_manual('', values=c(1, 2, 1, 2, 0), 
                         labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i])))

#Manipulated Data

laptop2 <- laptop
laptop2$price_dollars <- c(1600,1500,2250,2002,2000,1286.16,2400,1401.26,1685.59,2500,1219.89,1300.78,1575.09,1676.1,2264.4,2174.38,1293.15,1443.00,1290.93,1419.69,1300)
laptop2.lm <- lm(price_dollars ~ Cpu, data = laptop2)

#Second Graph

plot2 <- ggplot(data = laptop2, aes(x = Cpu, y = price_dollars)) +
  geom_segment(data = laptop2, aes(x = Cpu, xend = Cpu, y= predict(laptop2.lm, x = Cpu), yend = price_dollars,
                   color='a', shape = 'a', linetype= 'a'), 
               size = 1) +
  geom_segment(data = laptop2, aes(x = Cpu, xend = Cpu, y= mean(price_dollars), yend = price_dollars,
                   color='aa', shape = 'aa', linetype= 'aa'), 
               size = 1) +
  geom_smooth(data = laptop2, method = "lm", se = FALSE, aes(color = 'b',shape = 'b', linetype= 'b')) +
  geom_line(data = laptop2, aes(y = mean(price_dollars), color = "c", shape = "c", linetype = "c"), size = 1.5) +
  geom_point(data = laptop2, aes(color='d', shape = 'd', linetype= 'd'), size =2.5) +
  ylim(1000,3000) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual('', values=c("darkgreen","darkgray" ,"firebrick1","darkgray", "blue"), 
                     labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_shape_manual('', values = c(95,95,NA,95,19), 
                      labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_linetype_manual('', values=c(1, 2, 1, 2, 0), 
                         labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i])))

require(gridExtra)
grid.arrange(plot1, plot2, ncol=2)

```


```{r}
SSE2 <-  sum((laptop2$price_dollars - laptop2.lm$fit)^2)
SSR2 <-  sum((laptop2.lm$fit - mean(laptop2$price_dollars))^2)
SSTO2 <-  sum((laptop2$price_dollars - mean(laptop2$price_dollars))^2)

laptops3 <- data.frame(
  Type = c("SSE","SSR","SSTO"),
  "1st Graph" = c(SSE,SSR,SSTO),
  "2nd Graph" = c(SSE2,SSR2,SSTO2))

panderOptions('digits',12)
panderOptions('round',2)

laptops3 %>% 
  pander()

```
</center>

As you can see, when the value of SSE is small and SSR is large we have a better fit to the regression model. When SSE is large and SSR is small, we have poorer fit. While this is insightful, there is yet, another, better indicator of the fit of the model and it is using these numbers that we have calculated. See part 3 to learn about $R^2$.


### Part 3

#### R-squared

Simply put, $R^2$, or the square of the correlation, is a measure of how well the regression line fits the data. It is calculated by SSR/SSTO. Or:

$$
R^2 = \frac{SSR}{SSTO} = 1-\frac{SSE}{SSTO}
$$

Below are the graph's from part 2 and a table that now includes $R^2$ for the data set that we have been working with and the manipulated data. 

<center>
```{r message=FALSE, warning=FALSE, echo = FALSE}

# First Graph

plot1 <- ggplot(data = laptop, aes(x = Cpu, y = price_dollars)) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= predict(laptop.lm, x = Cpu), yend = price_dollars,
                   color='a', shape = 'a', linetype= 'a'), 
               size = 1, data = laptop) +
  geom_segment(aes(x = Cpu, xend = Cpu, y= mean(price_dollars), yend = price_dollars,
                   color='aa', shape = 'aa', linetype= 'aa'), 
               size = 1, data = laptop) +
  geom_smooth(method = "lm", se = FALSE, aes(color = 'b',shape = 'b', linetype= 'b')) +
  geom_line(aes(y = mean(price_dollars), color = "c", shape = "c", linetype = "c"), size = 1.5) +
  geom_point(aes(color='d', shape = 'd', linetype= 'd'), size =2.5) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual('', values=c("darkgreen","darkgray" ,"firebrick1","darkgray", "blue"), 
                     labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_shape_manual('', values = c(95,95,NA,95,19), 
                      labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_linetype_manual('', values=c(1, 2, 1, 2, 0), 
                         labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i])))

#Manipulated Data

laptop2 <- laptop
laptop2$price_dollars <- c(1600,1500,2250,2002,2000,1286.16,2400,1401.26,1685.59,2500,1219.89,1300.78,1575.09,1676.1,2264.4,2174.38,1293.15,1443.00,1290.93,1419.69,1300)
laptop2.lm <- lm(price_dollars ~ Cpu, data = laptop2)

#Second Graph

plot2 <- ggplot(data = laptop2, aes(x = Cpu, y = price_dollars)) +
  geom_segment(data = laptop2, aes(x = Cpu, xend = Cpu, y= predict(laptop2.lm, x = Cpu), yend = price_dollars,
                   color='a', shape = 'a', linetype= 'a'), 
               size = 1) +
  geom_segment(data = laptop2, aes(x = Cpu, xend = Cpu, y= mean(price_dollars), yend = price_dollars,
                   color='aa', shape = 'aa', linetype= 'aa'), 
               size = 1) +
  geom_smooth(data = laptop2, method = "lm", se = FALSE, aes(color = 'b',shape = 'b', linetype= 'b')) +
  geom_line(data = laptop2, aes(y = mean(price_dollars), color = "c", shape = "c", linetype = "c"), size = 1.5) +
  geom_point(data = laptop2, aes(color='d', shape = 'd', linetype= 'd'), size =2.5) +
  ylim(1000,3000) +
  labs(x = "CPU Size(GHz)",
       y = "Price(Dollars)",
       title = "The Price of Laptops by CPU Size",
       color = "") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_color_manual('', values=c("darkgreen","darkgray" ,"firebrick1","darkgray", "blue"), 
                     labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_shape_manual('', values = c(95,95,NA,95,19), 
                      labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i]))) +
  scale_linetype_manual('', values=c(1, 2, 1, 2, 0), 
                         labels=c(expression(Y[i]-hat(Y)[i]),expression(Y[i]-bar(Y)), expression(hat(Y)[i]), expression(bar(Y)), expression(Y[i])))

require(gridExtra)
grid.arrange(plot1, plot2, ncol=2)

```


```{r}
SSE2 <-  sum((laptop2$price_dollars - laptop2.lm$fit)^2)
SSR2 <-  sum((laptop2.lm$fit - mean(laptop2$price_dollars))^2)
SSTO2 <-  sum((laptop2$price_dollars - mean(laptop2$price_dollars))^2)

laptops4 <- data.frame(
  Type = c("SSE","SSR","SSTO", "R2","r"),
  "1st Graph" = c(SSE,SSR,SSTO,SSR/SSTO,sqrt(SSR/SSTO)),
  "2nd Graph" = c(SSE2,SSR2,SSTO2,SSR2/SSTO2,sqrt(SSR2/SSTO2)))

panderOptions('digits',12)
panderOptions('round',2)

laptops4 %>% 
  pander()

```
</center>

As you can see, the 2nd graphs $R^2$ is greater than the 1st graph's $R^2$. This is because as the data points, $Y_i$, approach the predicted average, $R^2$ approaches 1. Conversely as $R^2$ approaches 0, you could expect the fit of the regression line to be very poor. 

$R^2$ gives us an idea of how well the regression line fits the data; the p-value give us an idea of whether or not the slope of the regression line is significant. This means that $R^2$ could be perfect, in terms of being close to 1 with the points being near the regression line, but the slope of the regression might close to 0 and therefore insignificant. In terms of analysis, we want to first make sure that the slope is significant and then check our $R^2$ to determine the fit of the line. 









